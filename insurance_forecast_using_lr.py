# -*- coding: utf-8 -*-
"""Insurance forecast using LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NiQm09AcTskpvMcMpFgRv8RM4-wBO44v
"""

import pandas as pd
import numpy as np
df= pd.read_csv("/content/drive/MyDrive/insurance.csv")

df.head(10)

df.describe()

df.info()

df['sex']=np.where(df['sex']=="female",1,0)
df['smoker']=np.where(df['smoker']=="yes",1,0)
df.head() #coverting the categorical text into numerical value

df=pd.get_dummies(df,columns=["region"],drop_first=True)

#data quality check
round(100*(df.isnull().sum()/len(df)),2).sort_values(ascending =False)



#percentage of missing values in each rows
round(100*(df.isnull().sum(axis=1)/len(df)),2).sort_values(ascending =False)[:5]

#duplicate check
df_dub= df.copy()
df_dub.drop_duplicates(subset = None, inplace = True)

df_dub.shape

df.shape

"""After checking the duplicate values there is no difference between the drop dataframe and the original dataframe"""

#datacleaning
for col in df:
  print(df[col].value_counts(ascending=False),'\n\n\n')

# Check the datatypes before convertio
df.info()

df['sex'] = df['sex'].astype('category')
df['smoker']= df['smoker'].astype('category')
df['children']= df['children'].astype('category')

df_new = pd.get_dummies(df, drop_first=True)
df_new.info()

df_new.shape

"""Splitting the data
splitting the data to Train and test - we will split the data into Train and test(70:30)
We will use train_test_split method from sklearn package for this
"""

df.info()

from sklearn.model_selection import train_test_split
np.random.seed(0)
df_train, df_test = train_test_split(df_new,train_size  = 0.70,test_size =0.30, random_state = 100)

df_train.info()

bool_cols = df_train.select_dtypes(include='bool').columns
for col in bool_cols:
    df_train[col] = df_train[col].astype('uint8')
    df_test[col] = df_test[col].astype('uint8')

print("df_train dtypes after conversion:")
df_train.info()
print("\ndf_test dtypes after conversion:")
df_test.info()

df_train.columns

import seaborn as sns
import matplotlib.pyplot as plt

df_num = df_train[['age','bmi','charges']]
sns.pairplot(df_num, diag_kind = 'kde')
plt.show()



"""Insights
The above Pair-plot tells us that there is a Linear Relation between 'age','bmi' and 'Charges'


Visualising Categorical Variables
"""

df_train.info()

plt.figure(figsize=(25,10))
plt.subplot(2,2,1)
sns.boxplot(x= 'sex',y='charges',data = df)
plt.subplot(2,2,2)
sns.boxplot(x = 'children',y='charges',data = df)
plt.subplot(2,2,3)
sns.boxplot(x='smoker',y ='charges',data = df)

# Fix for the 'region' plot: Reconstruct the original region column for plotting
temp_df = df.copy() # Create a temporary copy to avoid modifying the original df
conditions = [
    temp_df['region_northwest'] == True,
    temp_df['region_southeast'] == True,
    temp_df['region_southwest'] == True
]
choices = ['northwest', 'southeast', 'southwest']
# 'northeast' is assumed to be the region when all other region dummies are False
temp_df['original_region'] = np.select(conditions, choices, default='northeast')

plt.subplot(2,2,4)
sns.boxplot(x='original_region', y ='charges',data = temp_df)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt
plt.figure(figsize =(25,10))
sns.heatmap(df_new.corr(),annot= True,cmap="RdBu")
plt.show()

#rescaling the features
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()

df_train.head()

df_train.columns

num_vars = ['age','bmi','charges']
df_train[num_vars] = scaler.fit_transform(df_train[num_vars])

df_train.head()

df_train.describe()

# #building a linear model
# dividing into X and Y sets for the model building
y_train = df_train.pop('charges')
X_train = df_train



"""# RFE
# Recursive feature elimination: We will be using the LinearRegression function from SciKit Learn for its compatibility with RFE (which is a utility from sklearn)
"""

from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
lm =LinearRegression()
lm.fit(X_train,y_train)
rfe = RFE(lm, n_features_to_select=6)
rfe = rfe.fit(X_train,y_train)

list(zip(X_train.columns,rfe.support_,rfe.ranking_))

col =X_train.columns[rfe.support_]
col

X_train.columns[~rfe.support_]

X_train_rfe= X_train[col]

list(zip(X_train.columns, [bool(s) for s in rfe.support_], [int(rank) for rank in rfe.ranking_]))

col = X_train.columns[rfe.support_]
col

X_train.columns[~rfe.support_]

X_train_rfe = X_train[col]

#building linear model using stats model
from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif['Features'] = X_train_rfe.columns
vif['VIF'] = [variance_inflation_factor(X_train_rfe.values,i)for i in range (X_train_rfe.shape[1])]
vif['VIF'] = round(vif['VIF'],2)
vif = vif.sort_values(by ="VIF",ascending = False)
vif

import statsmodels.api as sm
X_train_lm1 = sm.add_constant(X_train_rfe)
lr1= sm.OLS(y_train,X_train_lm1).fit()

lr1.params

print(lr1.summary())

