# -*- coding: utf-8 -*-
"""Heartdiseaseprediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_qfK3juo0XhoVTZqOzn3j8Hn95NtMYji
"""



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/drive/MyDrive/framingham.csv")

df.head()

df.info()

df.shape

df.describe()

"""# Task
Identify and display the number of missing values for each column in the DataFrame. Then, impute missing values in the numerical columns 'education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose' using an appropriate strategy (e.g., the mean or median of each respective column). Finally, verify that there are no remaining missing values in the DataFrame.

## Identify Missing Values

### Subtask:
Calculate and display the number of missing values for each column in the DataFrame to understand the extent of missing data.

**Reasoning**:
To identify missing values, I will calculate the sum of null values for each column in the DataFrame `df` and display them.
"""

df.isnull().sum()

"""## Impute Missing Values

### Subtask:
Impute missing values in the numerical columns 'education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose' using an appropriate strategy (e.g., the mean or median of each respective column).

### Reasoning:
For numerical columns, imputing missing values with the median is generally a robust strategy, especially if there are outliers, as the median is less sensitive to extreme values than the mean. The columns 'education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose' will be imputed using their respective medians.

**Reasoning**:
To impute the missing values in the specified numerical columns, I will iterate through each column, calculate its median, and then fill the null values in that column with its calculated median.
"""

columns_to_impute = ['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose']

for column in columns_to_impute:
    median_value = df[column].median()
    df[column].fillna(median_value, inplace=True)

print("Missing values after imputation:")
print(df[columns_to_impute].isnull().sum())

"""**Reasoning**:
The previous code block successfully imputed missing values but generated a `FutureWarning` due to `inplace=True` with chained assignment. To fix this, I will update the code to assign the result of `fillna` directly back to the DataFrame column, which is the recommended practice and avoids the warning. Additionally, after imputation, I need to verify that there are no remaining missing values across the entire DataFrame to complete the task.


"""

columns_to_impute = ['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose']

for column in columns_to_impute:
    median_value = df[column].median()
    df[column] = df[column].fillna(median_value)

print("Missing values after imputation:")
print(df[columns_to_impute].isnull().sum())

print("\nTotal missing values in the DataFrame after imputation:")
print(df.isnull().sum())

import matplotlib.pyplot as plt
import seaborn as sns

# Get numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Create box plots for each numerical column
plt.figure(figsize=(15, 20))
for i, column in enumerate(numerical_cols, 1):
    plt.subplot(len(numerical_cols) // 3 + 1, 3, i)
    sns.boxplot(y=df[column])
    plt.title(f'Box Plot of {column}')
    plt.ylabel('')
plt.tight_layout()
plt.show()

"""Summarize the steps taken to handle missing values and confirm the DataFrame is clean.

The task successfully identified and displayed the number of missing values for each column. Missing values in the specified numerical columns ('education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose') were imputed using the median strategy. Finally, it was verified that there are no remaining missing values in the DataFrame.


*   Initially, the DataFrame contained missing values in several numerical columns: 'education' (105 missing values), 'cigsPerDay' (29 missing values), 'BPMeds' (53 missing values), 'totChol' (50 missing values), 'BMI' (19 missing values), 'heartRate' (1 missing value), and 'glucose' (388 missing values).
*   Missing values in these columns were imputed using the median value of each respective column. This strategy was chosen for its robustness against outliers.
*   After the imputation process, a verification step confirmed that there are no remaining missing values across the entire DataFrame.


*   Using the median for imputation is a good practice when potential outliers might skew the mean, ensuring a more robust handling of missing numerical data.
*   For future analyses or larger datasets, consider exploring more sophisticated imputation methods like K-nearest neighbors (KNN) imputation or regression imputation, especially if the missingness is not completely at random, to potentially preserve more complex data relationships.

## Summary:

### Q&A
The task successfully identified and displayed the number of missing values for each column. Missing values in the specified numerical columns ('education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose') were imputed using the median strategy. Finally, it was verified that there are no remaining missing values in the DataFrame.

### Data Analysis Key Findings
*   Initially, the DataFrame contained missing values in several numerical columns: 'education' (105 missing values), 'cigsPerDay' (29 missing values), 'BPMeds' (53 missing values), 'totChol' (50 missing values), 'BMI' (19 missing values), 'heartRate' (1 missing value), and 'glucose' (388 missing values).
*   Missing values in these columns were imputed using the median value of each respective column. This strategy was chosen for its robustness against outliers.
*   After the imputation process, a verification step confirmed that there are no remaining missing values across the entire DataFrame.
*   **Box Plot Observations:**
    *   `totChol`: Shows a wide range and several outliers on the higher end, indicating some individuals with very high cholesterol levels.
    *   `sysBP`: Exhibits a relatively normal distribution but with a significant number of outliers in the higher systolic blood pressure range, suggesting a prevalence of hypertension.
    *   `diaBP`: Similar to `sysBP`, it has a good spread with some outliers, particularly on the higher side, indicating elevated diastolic blood pressure in certain cases.
    *   `BMI`: Appears to be right-skewed with a number of outliers on the higher end, reflecting a subset of individuals with high BMI.
    *   `heartRate`: Generally has a concentrated distribution around the median but also shows some outliers on both the lower and higher ends, representing individuals with unusually low or high heart rates.
    *   `glucose`: Displays a right-skewed distribution with a substantial number of high-value outliers, indicating a population segment with elevated glucose levels, potentially related to diabetes.

### Insights or Next Steps
*   Using the median for imputation is a good practice when potential outliers might skew the mean, ensuring a more robust handling of missing numerical data.
*   For future analyses or larger datasets, consider exploring more sophisticated imputation methods like K-nearest neighbors (KNN) imputation or regression imputation, especially if the missingness is not completely at random, to potentially preserve more complex data relationships.
*   The presence of numerous outliers in critical health indicators like `totChol`, `sysBP`, `diaBP`, `BMI`, and `glucose` suggests that these features might require further investigation or specialized outlier handling techniques during model training to avoid undue influence on the model.

## Kernel State
Here are some of the notable variables in the kernel:
Variable #1
name: `df`, type: `DataFrame`
value:
```
      male  age  education  currentSmoker  cigsPerDay  ...  diaBP    BMI  
0        1   39        4.0              0         0.0  ...   70.0  26.97   
1        0   46        2.0              0         0.0  ...   81.0  28.73   
2        1   48        1.0              1        20.0  ...   80.0  25.34   
3        0   61        3.0              1        30.0  ...   95.0  28.58   
4        0   46        3.0              1        23.0  ...   84.0  23.10   
...    ...  <TRUNCATED original_length=1406>
```
Variable #2
name: `column`, type: `str`
value: `'TenYearCHD'`
Variable #3
name: `columns_to_impute`, type: `list`
value: `['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose']`
Variable #4
name: `i`, type: `int`
value: `16`
Variable #5
name: `median_value`, type: `float`
value: `78.0`
Here are some of the files available in the kernel:
* /content/.config/active_config
* /content/.config/.last_survey_prompt.yaml
* /content/.config/gce
* /content/.config/.last_update_check.json
* /content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db
* /content/.config/default_configs.db
* /content/.config/config_sentinel
* /content/.config/.last_opt_in_prompt.yaml
* /content/sample_data/README.md
* /content/sample_data/anscombe.json
* /content/sample_data/mnist_test.csv
* /content/sample_data/california_housing_test.csv
* /content/sample_data/california_housing_train.csv
* /content/sample_data/mnist_train_small.csv
* /content/drive/MyDrive/measurements.csv
* /content/drive/MyDrive/insurance.csv
* /content/drive/MyDrive/cars_data.csv
* /content/drive/MyDrive/framingham.csv
* /content/.config/configurations/config_default
* /content/drive/MyDrive/ Dataset/Data Dictionary.xls
* /content/drive/MyDrive/ Dataset/laptopData.csv

## Summary:

### Q&A
The task successfully identified and displayed the number of missing values for each column. Missing values in the specified numerical columns ('education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', and 'glucose') were imputed using the median strategy. Finally, it was verified that there are no remaining missing values in the DataFrame.

### Data Analysis Key Findings
*   Initially, the DataFrame contained missing values in several numerical columns: 'education' (105 missing values), 'cigsPerDay' (29 missing values), 'BPMeds' (53 missing values), 'totChol' (50 missing values), 'BMI' (19 missing values), 'heartRate' (1 missing value), and 'glucose' (388 missing values).
*   Missing values in these columns were imputed using the median value of each respective column. This strategy was chosen for its robustness against outliers.
*   After the imputation process, a verification step confirmed that there are no remaining missing values across the entire DataFrame.
*   **Box Plot Observations:**
    *   `totChol`: Shows a wide range and several outliers on the higher end, indicating some individuals with very high cholesterol levels.
    *   `sysBP`: Exhibits a relatively normal distribution but with a significant number of outliers in the higher systolic blood pressure range, suggesting a prevalence of hypertension.
    *   `diaBP`: Similar to `sysBP`, it has a good spread with some outliers, particularly on the higher side, indicating elevated diastolic blood pressure in certain cases.
    *   `BMI`: Appears to be right-skewed with a number of outliers on the higher end, reflecting a subset of individuals with high BMI.
    *   `heartRate`: Generally has a concentrated distribution around the median but also shows some outliers on both the lower and higher ends, representing individuals with unusually low or high heart rates.
    *   `glucose`: Displays a right-skewed distribution with a substantial number of high-value outliers, indicating a population segment with elevated glucose levels, potentially related to diabetes.

### Insights or Next Steps
*   Using the median for imputation is a good practice when potential outliers might skew the mean, ensuring a more robust handling of missing numerical data.
*   For future analyses or larger datasets, consider exploring more sophisticated imputation methods like K-nearest neighbors (KNN) imputation or regression imputation, especially if the missingness is not completely at random, to potentially preserve more complex data relationships.
*   The presence of numerous outliers in critical health indicators like `totChol`, `sysBP`, `diaBP`, `BMI`, and `glucose` suggests that these features might require further investigation or specialized outlier handling techniques during model training to avoid undue influence on the model.

## Kernel State
Here are some of the notable variables in the kernel:
Variable #1
name: `df`, type: `DataFrame`
value:
```
      male  age  education  currentSmoker  cigsPerDay  ...  diaBP    BMI  
0        1   39        4.0              0         0.0  ...   70.0  26.97   
1        0   46        2.0              0         0.0  ...   81.0  28.73   
2        1   48        1.0              1        20.0  ...   80.0  25.34   
3        0   61        3.0              1        30.0  ...   95.0  28.58   
4        0   46        3.0              1        23.0  ...   84.0  23.10   
...    ...  <TRUNCATED original_length=1406>
```
Variable #2
name: `column`, type: `str`
value: `'TenYearCHD'`
Variable #3
name: `columns_to_impute`, type: `list`
value: `['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate', 'glucose']`
Variable #4
name: `i`, type: `int`
value: `16`
Variable #5
name: `median_value`, type: `float`
value: `78.0`
Here are some of the files available in the kernel:
* /content/.config/active_config
* /content/.config/.last_survey_prompt.yaml
* /content/.config/gce
* /content/.config/.last_update_check.json
* /content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db
* /content/.config/default_configs.db
* /content/.config/config_sentinel
* /content/.config/.last_opt_in_prompt.yaml
* /content/sample_data/README.md
* /content/sample_data/anscombe.json
* /content/sample_data/mnist_test.csv
* /content/sample_data/california_housing_test.csv
* /content/sample_data/california_housing_train.csv
* /content/sample_data/mnist_train_small.csv
* /content/drive/MyDrive/measurements.csv
* /content/drive/MyDrive/insurance.csv
* /content/drive/MyDrive/cars_data.csv
* /content/drive/MyDrive/framingham.csv
* /content/.config/configurations/config_default
* /content/drive/MyDrive/ Dataset/Data Dictionary.xls
* /content/drive/MyDrive/ Dataset/laptopData.csv

## Final Task

### Subtask:
Provide a summary of the data preparation steps and initial insights.

## Summary:

### Q&A
The task successfully identified and addressed missing values in the dataset. Missing values in numerical columns (`education`, `cigsPerDay`, `BPMeds`, `totChol`, `BMI`, `heartRate`, and `glucose`) were imputed using the median strategy. Following imputation, the DataFrame was verified to be clean, with no remaining missing values. The box plots provided initial insights into the distribution and presence of outliers for several key health indicators.

### Data Analysis Key Findings
*   **Initial Missing Values**: The DataFrame initially contained missing values in the following numerical columns:
    *   `education`: 105 missing values
    *   `cigsPerDay`: 29 missing values
    *   `BPMeds`: 53 missing values
    *   `totChol`: 50 missing values
    *   `BMI`: 19 missing values
    *   `heartRate`: 1 missing value
    *   `glucose`: 388 missing values
*   **Imputation Strategy**: Missing values in these columns were imputed using the median value of each respective column, a robust strategy against outliers.
*   **Data Cleanliness**: After imputation, the DataFrame was confirmed to have no remaining missing values.
*   **Box Plot Observations**:
    *   `totChol`: Exhibited a wide range and several outliers on the higher end, suggesting individuals with very high cholesterol.
    *   `sysBP`: Showed a relatively normal distribution but with a significant number of outliers in the higher range, indicating a prevalence of hypertension.
    *   `diaBP`: Similar to `sysBP`, it had a good spread with some outliers on the higher side, pointing to elevated diastolic blood pressure in certain cases.
    *   `BMI`: Appeared right-skewed with multiple high-end outliers, reflecting a subset of individuals with high BMI.
    *   `heartRate`: Generally concentrated around the median but also displayed outliers on both lower and higher ends, representing individuals with unusually low or high heart rates.
    *   `glucose`: Presented a right-skewed distribution with a substantial number of high-value outliers, indicating a population segment with elevated glucose levels, potentially related to diabetes.

### Insights or Next Steps
*   The use of the median for imputation is a sound approach for handling missing numerical data, especially given the presence of potential outliers, as it ensures robustness against extreme values.
*   For future analyses, especially with larger datasets or if the missingness pattern is complex, consider exploring advanced imputation methods like K-nearest neighbors (KNN) or regression imputation to potentially preserve more intricate data relationships.
*   The significant presence of outliers in critical health indicators (`totChol`, `sysBP`, `diaBP`, `BMI`, `glucose`) suggests that these features might require further investigation or specialized outlier handling techniques during model training to prevent undue influence on the model's performance.

# Task
Build and evaluate a logistic regression model to predict `TenYearCHD` using the preprocessed data, including splitting the data into training and testing sets, training the model, evaluating its performance with metrics like accuracy, precision, recall, F1-score, and a confusion matrix, and finally summarizing the model's performance and key findings.

## Prepare Data for Modeling

### Subtask:
Separate features (X) and the target variable (y), then split the data into training and testing sets for model development and evaluation.

**Reasoning**:
First, I will separate the features (X) and the target variable (y) from the DataFrame `df`. Then, I will split the data into training and testing sets using `train_test_split` to prepare it for model development and evaluation.
"""

from sklearn.model_selection import train_test_split

X = df.drop('TenYearCHD', axis=1)
y = df['TenYearCHD']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""**Reasoning**:
Now that the data is split into training and testing sets, I will train a Logistic Regression model on the training data (`X_train`, `y_train`) to learn the relationship between the features and the target variable.


"""

from sklearn.linear_model import LogisticRegression

# Initialize the Logistic Regression model
model = LogisticRegression(random_state=42, solver='liblinear') # Using 'liblinear' solver for better convergence on smaller datasets

# Train the model
model.fit(X_train, y_train)

print("Logistic Regression model trained successfully.")

"""**Reasoning**:
Now that the Logistic Regression model has been trained, the next step is to evaluate its performance on the test set. I will use the trained model to make predictions on `X_test` and then calculate various classification metrics to assess the model's effectiveness.


"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

print("Model Evaluation:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1-Score: {f1:.4f}")
print("Confusion Matrix:\n", conf_matrix)

"""### Model Performance Summary

The Logistic Regression model was trained to predict the `TenYearCHD` outcome. Here's a summary of its performance on the test set:

*   **Accuracy:** 0.8491 (84.91%)
    *   This indicates that the model correctly predicted the outcome for approximately 84.91% of the cases in the test set. While seemingly high, accuracy can be misleading in imbalanced datasets.

*   **Precision:** 0.5333
    *   Of all the instances predicted as positive for `TenYearCHD`, 53.33% were actually positive. This means that a little over half of the model's positive predictions were correct.

*   **Recall:** 0.0620
    *   The model only identified 6.20% of all actual positive cases of `TenYearCHD`. This is a very low recall, suggesting the model is struggling to detect individuals who truly have a 10-year risk of coronary heart disease.

*   **F1-Score:** 0.1111
    *   The F1-score, which is the harmonic mean of precision and recall, is very low. This further confirms that the model's overall effectiveness in classifying the positive class is poor.

*   **Confusion Matrix:**
    ```
    [[712   7]
     [121   8]]
    ```
    *   **True Negatives (TN):** 712 (Correctly predicted no CHD)
    *   **False Positives (FP):** 7 (Incorrectly predicted CHD when there was none)
    *   **False Negatives (FN):** 121 (Incorrectly predicted no CHD when there was CHD)
    *   **True Positives (TP):** 8 (Correctly predicted CHD)

### Key Findings and Insights

1.  **Imbalance Issue:** The confusion matrix clearly shows a significant class imbalance in the target variable, where the number of negative cases (no CHD) is much higher than positive cases (CHD). This is evident from the high number of True Negatives (712) compared to True Positives (8).
2.  **Poor Positive Class Prediction:** The model is very good at predicting the negative class (high True Negatives, low False Positives), which inflates the overall accuracy. However, its performance on the positive class (predicting `TenYearCHD`) is extremely poor, as indicated by the low recall (only 8 out of 129 actual positive cases were identified) and a very low F1-score.
3.  **Risk of Missed Cases:** A low recall for predicting CHD means that a large number of individuals who are at risk are being missed by the model. In a medical context, this could have serious implications.

### Next Steps

*   **Address Class Imbalance:** Techniques such as oversampling the minority class (e.g., SMOTE), undersampling the majority class, or using class weights in the model training should be explored.
*   **Feature Engineering/Selection:** Further investigation into feature relevance and potentially creating new features might improve the model's ability to distinguish between classes.
*   **Different Models:** Explore other classification algorithms that might be more robust to imbalanced datasets or offer better performance for this specific problem (e.g., Random Forest, Gradient Boosting, SVM).
*   **Threshold Adjustment:** While current metrics are poor, adjusting the classification threshold for the positive class predictions could be considered, though often with a trade-off between precision and recall.

## Final Task

### Subtask:
Summarize the performance of the logistic regression model and discuss key findings.

## Summary:

### Q&A
*   **What is the performance of the logistic regression model?**
    The logistic regression model achieved an accuracy of 0.8491 (84.91%) on the test set. However, its precision for predicting `TenYearCHD` was 0.5333, while its recall was notably low at 0.0620. The F1-score, which balances precision and recall, was also very low at 0.1111. The confusion matrix showed 712 True Negatives, 7 False Positives, 121 False Negatives, and 8 True Positives.

*   **What are the key findings from the model's performance?**
    The model's high accuracy is misleading due to a significant class imbalance, where negative cases (no CHD) heavily outweigh positive cases (CHD). It performs well in predicting the negative class (712 True Negatives, 7 False Positives) but struggles significantly with the positive class, identifying only 8 out of 129 actual positive cases. This low recall means a large number of individuals at risk of CHD are missed.

### Data Analysis Key Findings
*   The dataset was split into training and testing sets with an 80/20 ratio, resulting in 3390 samples for training and 848 for testing.
*   The Logistic Regression model achieved an accuracy of 0.8491 on the test set.
*   Precision for the positive class was 0.5333, indicating that approximately 53.33% of positive predictions were correct.
*   Recall for the positive class was very low at 0.0620, meaning the model only identified 6.20% of actual positive cases.
*   The F1-score, a harmonic mean of precision and recall, was 0.1111, indicating poor overall performance for the positive class.
*   The confusion matrix revealed 712 True Negatives, 7 False Positives, 121 False Negatives, and only 8 True Positives, highlighting a significant class imbalance and the model's difficulty in detecting the minority class.

### Insights or Next Steps
*   Address the pronounced class imbalance in the target variable using techniques like oversampling (e.g., SMOTE), undersampling, or applying class weights during model training.
*   Explore alternative machine learning models that are more robust to imbalanced datasets, such as Random Forest, Gradient Boosting, or Support Vector Machines, to potentially improve the detection of the minority class.
"""